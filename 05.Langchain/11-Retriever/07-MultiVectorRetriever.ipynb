{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d96a9939",
   "metadata": {},
   "source": [
    "# 다중 벡터저장소 검색기(MultiVectorRetriever)\n",
    "\n",
    "LangChain에서는 문서를 다양한 상황에서 효율적으로 쿼리할 수 있는 특별한 기능, 바로 `MultiVectorRetriever`를 제공합니다. 이 기능을 사용하면 문서를 여러 벡터로 저장하고 관리할 수 있어, 정보 검색의 정확도와 효율성을 대폭 향상시킬 수 있습니다. 오늘은 이 `MultiVectorRetriever`를 활용해 문서당 여러 벡터를 생성하는 몇 가지 방법을 살펴보겠습니다.\n",
    "\n",
    "**문서당 여러 벡터 생성 방법 소개**\n",
    "\n",
    "1. **작은 청크 생성**: 문서를 더 작은 단위로 나눈 후, 각 청크에 대해 별도의 임베딩을 생성합니다. 이 방식을 사용하면 문서의 특정 부분에 좀 더 세심한 주의를 기울일 수 있습니다. 이 과정은 `ParentDocumentRetriever`를 통해 구현할 수 있어, 세부 정보에 대한 탐색이 용이해집니다.\n",
    "\n",
    "2. **요약 임베딩**: 각 문서의 요약을 생성하고, 이 요약으로부터 임베딩을 만듭니다. 이 요약 임베딩은 문서의 핵심 내용을 신속하게 파악하는 데 큰 도움이 됩니다. 문서 전체를 분석하는 대신 핵심적인 요약 부분만을 활용하여 효율성을 극대화할 수 있습니다.\n",
    "\n",
    "3. **가설 질문 활용**: 각 문서에 대해 적합한 가설 질문을 만들고, 이 질문에 기반한 임베딩을 생성합니다. 특정 주제나 내용에 대해 깊이 있는 탐색을 원할 때 이 방법이 유용합니다. 가설 질문은 문서의 내용을 다양한 관점에서 접근하게 해주며, 더 광범위한 이해를 가능하게 합니다.\n",
    "\n",
    "4. **수동 추가 방식**: 사용자가 문서 검색 시 고려해야 할 특정 질문이나 쿼리를 직접 추가할 수 있습니다. 이 방법을 통해 사용자는 검색 과정에서 보다 세밀한 제어를 할 수 있으며, 자신의 요구 사항에 맞춘 맞춤형 검색이 가능해집니다.\n",
    "\n",
    "`MultiVectorRetriever`를 통해 이러한 다양한 접근 방식을 유연하게 활용함으로써, 사용자는 필요한 정보를 보다 정확하고 빠르게 찾을 수 있습니다. LangChain의 이 기능은 정보 검색 작업을 보다 효과적으로 만들어주는 훌륭한 도구입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384bc267",
   "metadata": {},
   "source": [
    "## 사용 방법\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b574ec5",
   "metadata": {},
   "source": [
    "텍스트 파일에서 데이터를 로드하고, 로드된 문서들을 지정된 크기로 분할하는 전처리 과정을 수행합니다.\n",
    "\n",
    "분할된 문서들은 추후 벡터화 및 검색 등의 작업에 사용될 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d6c52a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "loaders = [\n",
    "    # 첫 번째 데이터를 로드합니다.\n",
    "    TextLoader(\"./data/ai-story.txt\"),\n",
    "    # 두 번째 데이터를 로드합니다.\n",
    "    TextLoader(\"./data/appendix-keywords.txt\"),\n",
    "]\n",
    "docs = []  # 빈 문서 리스트를 초기화합니다.\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())  # 각 로더에서 문서를 로드하여 docs 리스트에 추가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453afd1e",
   "metadata": {},
   "source": [
    "데이터로부터 로드한 원본 도큐먼트는 `docs` 변수에 담았습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b671d6e",
   "metadata": {},
   "source": [
    "## 작은 청크 생성\n",
    "\n",
    "대용량 정보를 검색하는 경우, 더 작은 단위로 정보를 임베딩하는 것이 유용할 수 있습니다.\n",
    "\n",
    "이를 통해 임베딩은 의미론적 의미를 최대한 근접하게 포착하면서도, 가능한 한 많은 맥락을 하위 단계로 전달할 수 있습니다.\n",
    "\n",
    "`ParentDocumentRetriever`가 수행하는 작업이 바로 이것입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c7f0dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bc51c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adc132d3-6831-4e16-aecb-580fe086366b',\n",
       " '3c559b3b-c193-40a0-ae67-4fdc2ce4d620']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 자식 청크를 인덱싱하는 데 사용할 벡터 저장소\n",
    "# universally unique identifier (UUID)를 사용하여 문서를 식별합니다.\n",
    "import uuid\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "# 부모 문서의 저장소 계층\n",
    "store = InMemoryByteStore()\n",
    "\n",
    "id_key = \"doc_id\"\n",
    "# 검색기 (시작 시 비어 있음)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "# 문서 ID를 생성합니다.\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "# 두개의 생성된 id를 확인합니다.\n",
    "doc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa3e771",
   "metadata": {},
   "source": [
    "여기서 큰 청크로 분할하기 위한 `parent_text_splitter`\n",
    "\n",
    "더 작은 청크로 분할하기 위한 `child_text_splitter` 를 정의합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bc95363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RecursiveCharacterTextSplitter 객체를 생성합니다.\n",
    "parent_text_splitter = RecursiveCharacterTextSplitter(chunk_size=4000)\n",
    "\n",
    "# 더 작은 청크를 생성하는 데 사용할 분할기\n",
    "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e48a98",
   "metadata": {},
   "source": [
    "- `parent_text_splitter`를 사용하여 문서를 큰 청크 단위로 분할합니다.\n",
    "- 각 문서의 메타데이터에 `\"doc_id\"`를 키로 하고 생성한 `uuid` 를 입력합니다.\n",
    "- 최종적으로 `parent_docs` 리스트에는 원본 문서들이 큰 단위로 분할된 하위 문서들이 저장되며, 각 문서에는 원본 문서의 ID가 메타데이터로 포함됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4437c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_docs = []\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    _id = doc_ids[i]  # 현재 문서의 ID를 가져옵니다.\n",
    "    # 현재 문서를 하위 문서로 분할합니다.\n",
    "    parent_doc = parent_text_splitter.split_documents([doc])\n",
    "    for _doc in parent_doc:  # 분할된 문서에 대해 반복합니다.\n",
    "        # 문서의 메타데이터에 ID를 저장합니다.\n",
    "        _doc.metadata[id_key] = _id\n",
    "    parent_docs.extend(parent_doc)  # 분할된 문서를 리스트에 추가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f680da0",
   "metadata": {},
   "source": [
    "- `child_text_splitter`를 사용하여 문서를 더 작은 청크로 분할합니다.\n",
    "- 각 문서의 메타데이터에 `\"doc_id\"`를 키로 하고 생성한 `uuid` 를 입력합니다. 이는 작게 분할된 청크에 문서의 ID 를 부여하기 위함입니다.\n",
    "- 최종적으로 `child_docs` 리스트에는 원본 문서들이 작게 분할된 하위 문서들이 저장되며, 각 하위 문서에는 원본 문서의 ID가 메타데이터로 포함됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e56afe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "child_docs = []  # 하위 문서를 저장할 리스트를 초기화합니다.\n",
    "for i, doc in enumerate(docs):\n",
    "    _id = doc_ids[i]  # 현재 문서의 ID를 가져옵니다.\n",
    "    # 현재 문서를 하위 문서로 분할합니다.\n",
    "    child_doc = child_text_splitter.split_documents([doc])\n",
    "    for _doc in child_doc:  # 분할된 하위 문서에 대해 반복합니다.\n",
    "        # 하위 문서의 메타데이터에 ID를 저장합니다.\n",
    "        _doc.metadata[id_key] = _id\n",
    "    child_docs.extend(child_doc)  # 분할된 하위 문서를 리스트에 추가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a346a79",
   "metadata": {},
   "source": [
    "각각 분할된 청크의 수를 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dfd9565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분할된 parent_docs의 개수: 4\n",
      "분할된 child_docs의 개수: 54\n"
     ]
    }
   ],
   "source": [
    "print(f\"분할된 parent_docs의 개수: {len(parent_docs)}\")\n",
    "print(f\"분할된 child_docs의 개수: {len(child_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da6ab64",
   "metadata": {},
   "source": [
    "벡터저장소에 새롭게 생성한 작게 쪼개진 하위문서 집합을 추가합니다.\n",
    "\n",
    "다음으로는 상위 문서는 생성한 UUID 와 맵핑하여 `docstore` 에 추가합니다.\n",
    "\n",
    "- `mset()` 메서드를 통해 문서 ID와 문서 내용을 key-value 쌍으로 문서 저장소에 저장합니다.\n",
    "\n",
    "(예시) `list(zip(doc_ids, docs))[0]`\n",
    "\n",
    "```\n",
    "('36d475a5-9f1a-40ab-aeb1-ba720fa229d8',\n",
    " Document(page_content='Scikit Learn\\n\\nScikit-learn은 Python 언어를 위한 또 다른 핵심 라이브러리로, 기계 학습의 다양한 알고리즘을 구현하기 위해 설계되었습니다. 이 라이브러리는 2007년 David Cournapeau에 의해 프로젝트가 시작되었으며, 그 후로 커뮤니티의 광범위한 기여를 받아 현재까지 발전해왔습니다. ...))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a291a760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 저장소에 하위 문서를 추가합니다.\n",
    "retriever.vectorstore.add_documents(parent_docs)\n",
    "retriever.vectorstore.add_documents(child_docs)\n",
    "\n",
    "# 문서 저장소에 문서 ID와 문서를 매핑하여 저장합니다.\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a644c1",
   "metadata": {},
   "source": [
    "주어진 키워드에 대한 유사도 검색을 수행합니다. 가장 유사도가 높은 첫 번째 문서 조각을 출력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0d9ba0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'doc_id': 'adc132d3-6831-4e16-aecb-580fe086366b', 'source': './data/ai-story.txt'}, page_content='Word2Vec의 성공 이후, 이와 유사한 다른 단어 임베딩 기법들도 개발되었습니다. 그러나 Word2Vec은 그 간결함과 효율성, 높은 성능으로 인해 여전히 광범위하게 사용되며, NLP 분야에서 기본적인 도구로 자리 잡았습니다. Word2Vec는 단순한 텍스트 데이터를 통해 복잡한 언어의 의미 구조를 학습할 수 있는 강력한 방법을 제공함으로써, 컴퓨터가 인간 언어를 이해하는 방식을 혁신적으로 개선하였습니다.'),\n",
       " Document(metadata={'doc_id': 'adc132d3-6831-4e16-aecb-580fe086366b', 'source': './data/ai-story.txt'}, page_content='Word2Vec은 크게 두 가지 모델 아키텍처로 구성됩니다: Continuous Bag-of-Words (CBOW)와 Skip-Gram입니다. CBOW 모델은 주변 단어(맥락)를 기반으로 특정 단어를 예측하는 반면, Skip-Gram 모델은 하나의 단어로부터 주변 단어들을 예측합니다. 두 모델 모두 딥러닝이 아닌, 단순화된 신경망 구조를 사용하여 대규모 텍스트 데이터에서 학습할 수 있으며, 매우 효율적입니다.'),\n",
       " Document(metadata={'doc_id': 'adc132d3-6831-4e16-aecb-580fe086366b', 'source': './data/ai-story.txt'}, page_content='Word2Vec의 벡터 표현은 다양한 NLP 작업에 활용될 수 있습니다. 예를 들어, 단어의 유사도 측정, 문장이나 문서의 벡터 표현 생성, 기계 번역, 감정 분석 등이 있습니다. 또한, 벡터 연산을 통해 단어 간의 의미적 관계를 추론하는 것이 가능해집니다. 예를 들어, \"king\" - \"man\" + \"woman\"과 같은 벡터 연산을 수행하면, 결과적으로 \"queen\"과 유사한 벡터를 가진 단어를 찾을 수 있습니다.'),\n",
       " Document(metadata={'doc_id': '3c559b3b-c193-40a0-ae67-4fdc2ce4d620', 'source': './data/appendix-keywords.txt'}, page_content='Crawling\\n\\n정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\\n예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\\n연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\\n\\nWord2Vec\\n\\n정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\\n예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\\n연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\\nLLM (Large Language Model)')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorstore의 유사도 검색을 수행합니다.\n",
    "retriever.vectorstore.similarity_search(\"Word2Vec 의 정의?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa14866",
   "metadata": {},
   "source": [
    "다음과 같이 `score_threshold` 를 추가하여 유사도 검색을 수행할 수 있습니다.\n",
    "\n",
    "유사도 검색 결과에 유사도 점수가 0.5 이상인 결과만 반환합니다.\n",
    "\n",
    "또한, `k` 의 계수도 지정할 수 있으며, `k` 는 검색되는 문서의 개수를 의미합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "562e5b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'doc_id': 'adc132d3-6831-4e16-aecb-580fe086366b', 'source': './data/ai-story.txt'}, page_content='Word2Vec의 성공 이후, 이와 유사한 다른 단어 임베딩 기법들도 개발되었습니다. 그러나 Word2Vec은 그 간결함과 효율성, 높은 성능으로 인해 여전히 광범위하게 사용되며, NLP 분야에서 기본적인 도구로 자리 잡았습니다. Word2Vec는 단순한 텍스트 데이터를 통해 복잡한 언어의 의미 구조를 학습할 수 있는 강력한 방법을 제공함으로써, 컴퓨터가 인간 언어를 이해하는 방식을 혁신적으로 개선하였습니다.'),\n",
       "  0.8503175537825611),\n",
       " (Document(metadata={'doc_id': 'adc132d3-6831-4e16-aecb-580fe086366b', 'source': './data/ai-story.txt'}, page_content='Word2Vec은 크게 두 가지 모델 아키텍처로 구성됩니다: Continuous Bag-of-Words (CBOW)와 Skip-Gram입니다. CBOW 모델은 주변 단어(맥락)를 기반으로 특정 단어를 예측하는 반면, Skip-Gram 모델은 하나의 단어로부터 주변 단어들을 예측합니다. 두 모델 모두 딥러닝이 아닌, 단순화된 신경망 구조를 사용하여 대규모 텍스트 데이터에서 학습할 수 있으며, 매우 효율적입니다.'),\n",
       "  0.8438838584307975),\n",
       " (Document(metadata={'doc_id': 'adc132d3-6831-4e16-aecb-580fe086366b', 'source': './data/ai-story.txt'}, page_content='Word2Vec의 벡터 표현은 다양한 NLP 작업에 활용될 수 있습니다. 예를 들어, 단어의 유사도 측정, 문장이나 문서의 벡터 표현 생성, 기계 번역, 감정 분석 등이 있습니다. 또한, 벡터 연산을 통해 단어 간의 의미적 관계를 추론하는 것이 가능해집니다. 예를 들어, \"king\" - \"man\" + \"woman\"과 같은 벡터 연산을 수행하면, 결과적으로 \"queen\"과 유사한 벡터를 가진 단어를 찾을 수 있습니다.'),\n",
       "  0.8373008157213353)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score_threshold를 사용하여 유사도 검색을 수행합니다.\n",
    "retriever.vectorstore.similarity_search_with_relevance_scores(\n",
    "    \"Word2Vec 의 정의?\", score_threshold=0.5, k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ae24a1",
   "metadata": {},
   "source": [
    "`retriever` 객체의 `invoke` 메서드를 호출하여 관련된 문서를 검색합니다.\n",
    "\n",
    "주어진 쿼리에 관련성이 높은 문서를 검색합니다.\n",
    "\n",
    "여기서는 2개의 도큐먼트 안에 \"Word2Vec\" 의 정의가 포함되었기 때문에 2개의 문서 모두 검색 결과로 나왔습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a29f9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc = retriever.invoke(\"Word2Vec 의 정의?\")\n",
    "len(relevant_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8443ace4",
   "metadata": {},
   "source": [
    "첫 번째로 찾은 문서의 내용의 문자열 길이를 확인하면 문서 전체가 출력됨을 확인할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6db1f0e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7482"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retriever가 반환하는 문서의 길이를 확인합니다.\n",
    "len(retriever.invoke(\"Word2Vec 의 정의?\")[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc30896",
   "metadata": {},
   "source": [
    "리트리버(retriever)가 벡터 데이터베이스에서 기본적으로 수행하는 검색 유형은 유사도 검색입니다.\n",
    "\n",
    "LangChain Vector Stores는 [Max Marginal Relevance](https://api.python.langchain.com/en/latest/vectorstores/langchain_core.vectorstores.VectorStore.html#langchain_core.vectorstores.VectorStore.max_marginal_relevance_search)를 통한 검색도 지원하므로, 이를 대신 사용하고 싶다면 다음과 같이 `search_type` 속성을 설정하면 됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac718689",
   "metadata": {},
   "source": [
    "- `retriever` 객체의 `search_type` 속성을 `SearchType.mmr`로 설정합니다.\n",
    "  - 이는 검색 시 MMR(Maximal Marginal Relevance) 알고리즘을 사용하도록 지정하는 것입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c545fc19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7482"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers.multi_vector import SearchType\n",
    "\n",
    "# 검색 유형을 MMR(Maximal Marginal Relevance)로 설정\n",
    "retriever.search_type = SearchType.mmr\n",
    "\n",
    "# 검색어로 관련 문서를 검색하고, 첫 번째 문서의 페이지 내용 길이를 반환\n",
    "len(retriever.invoke(\"Word2Vec의 정의\")[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d961b6c",
   "metadata": {},
   "source": [
    "## 요약본(summary)을 벡터저장소에 저장\n",
    "\n",
    "요약은 종종 청크(chunk)의 내용을 보다 정확하게 추출할 수 있어 더 나은 검색 결과를 얻을 수 있습니다.\n",
    "\n",
    "여기서는 요약을 생성하는 방법과 이를 임베딩하는 방법에 대해 설명합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a161ea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}  # 입력 데이터에서 페이지 내용을 추출하는 함수\n",
    "    # 문서 요약을 위한 프롬프트 템플릿 생성\n",
    "    | ChatPromptTemplate.from_template(\n",
    "        \"Summarize the following document in Korean:\\n\\n{doc}\"\n",
    "    )\n",
    "    # OpenAI의 ChatGPT 모델을 사용하여 요약 생성 (최대 재시도 횟수: 0)\n",
    "    | ChatOpenAI(max_retries=0)\n",
    "    | StrOutputParser()  # 생성된 요약 결과를 문자열로 파싱\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b79165",
   "metadata": {},
   "source": [
    "- `chain.batch` 메서드를 사용하여 `docs` 리스트의 문서들을 일괄 처리합니다.\n",
    "- 여기서 `max_concurrency` 매개변수를 5로 설정하여 최대 5개의 문서를 동시에 처리할 수 있도록 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7fe3a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 동시성을 5로 설정하여 문서 배치 처리\n",
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a11208",
   "metadata": {},
   "source": [
    "요약된 내용을 출력하여 결과를 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "436bd00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit Learn은 Python 언어를 위한 머신 러닝 알고리즘 라이브러리로, 다양한 머신 러닝 작업을 지원하며 사용하기 쉬운 API를 제공한다. Scipy는 과학 계산을 위한 중심적인 파이썬 라이브러리로, 고급 수학 함수와 도구를 제공한다. Hugging Face는 NLP 분야에서 선도적인 역할을 하는 기술 회사로, Transformers 라이브러리를 통해 최신 NLP 기술에 접근할 수 있다. \"Attention Is All You Need\"는 변환기 모델을 소개한 논문으로, NLP 분야에 큰 영향을 미쳤다. ARIMA, SARIMA, SARIMAX는 시계열 데이터 분석과 예측을 위한 통계 모델로, 각각의 특징과 사용법이 있다. Word2Vec은 단어 임베딩 기법으로, 단어의 의미를 벡터 공간에 매핑한다.\n"
     ]
    }
   ],
   "source": [
    "# 요약을 출력합니다.\n",
    "print(summaries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dda790bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "의미론적 검색은 사용자의 질의를 이해하여 관련된 결과를 반환하는 방식이며, 임베딩은 텍스트 데이터를 벡터로 변환하는 과정을 말합니다. 토큰은 텍스트를 작은 단위로 분할하는 것을 의미하고, 토크나이저는 이를 수행하는 도구입니다. 벡터스토어는 벡터 형식으로 데이터를 저장하는 시스템이며, SQL은 데이터베이스를 관리하는 프로그래밍 언어입니다. 데이터를 교환하는 파일 형식인 CSV와 JSON, 그리고 자연어 처리를 위한 트랜스포머와 HuggingFace 라이브러리도 소개되었습니다. 또한, 디지털 변환, 크롤링, Word2Vec, LLM, FAISS, 오픈 소스, 구조화된 데이터, 파서, TF-IDF, 딥러닝, 스키마, DataFrame, Attention 메커니즘, 판다스, GPT, InstructGPT, 키워드 검색, 페이지 랭크, 데이터 마이닝, 멀티모달 기술 등에 대한 설명이 포함됩니다.\n"
     ]
    }
   ],
   "source": [
    "# 요약을 출력합니다.\n",
    "print(summaries[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04da5865",
   "metadata": {},
   "source": [
    "`Chroma` 벡터 저장소를 초기화하여 자식 청크(child chunks)를 인덱싱합니다. 이때 `OpenAIEmbeddings`를 임베딩 함수로 사용합니다.\n",
    "\n",
    "- 문서 ID를 나타내는 키로 `\"doc_id\"`를 사용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbe2f51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요약 정보를 저장할 벡터 저장소를 생성합니다.\n",
    "vectorstore = Chroma(collection_name=\"summaries\",\n",
    "                     embedding_function=OpenAIEmbeddings())\n",
    "# 부모 문서를 저장할 저장소를 생성합니다.\n",
    "store = InMemoryByteStore()\n",
    "# 문서 ID를 저장할 키 이름을 지정합니다.\n",
    "id_key = \"doc_id\"\n",
    "# 검색기를 초기화합니다. (시작 시 비어 있음)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,  # 벡터 저장소\n",
    "    byte_store=store,  # 바이트 저장소\n",
    "    id_key=id_key,  # 문서 ID 키\n",
    ")\n",
    "# 문서 ID를 생성합니다.\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d0e2c6",
   "metadata": {},
   "source": [
    "요약된 문서와 메타데이터(여기서는 생성한 요약본에 대한 `Document ID` 입니다)를 저장합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cec99148",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_docs = [\n",
    "    # 요약된 내용을 페이지 콘텐츠로 하고, 문서 ID를 메타데이터로 포함하는 Document 객체를 생성합니다.\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(\n",
    "        summaries\n",
    "    )  # summaries 리스트의 각 요약과 인덱스에 대해 반복합니다.\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb07ac1",
   "metadata": {},
   "source": [
    "요약본의 문서의 개수는 원본 문서의 개수와 일치합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d36e4d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 요약본의 문서의 개수\n",
    "len(summary_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4400de8c",
   "metadata": {},
   "source": [
    "- `retriever.vectorstore.add_documents(summary_docs)`를 통해 `summary_docs`를 벡터 저장소에 추가합니다.\n",
    "- `retriever.docstore.mset(list(zip(doc_ids, docs)))`를 사용하여 `doc_ids`와 `docs`를 매핑하여 문서 저장소에 저장합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df7ce3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.vectorstore.add_documents(\n",
    "    summary_docs\n",
    ")  # 요약된 문서를 벡터 저장소에 추가합니다.\n",
    "\n",
    "# 문서 ID와 문서를 매핑하여 문서 저장소에 저장합니다.\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f120d85",
   "metadata": {},
   "source": [
    "다음으로는 원본 청크(chunk) 데이터를 벡터 저장소에 추가하는 코드입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da43027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RecursiveCharacterTextSplitter 객체를 생성합니다.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
    "\n",
    "split_docs = []\n",
    "split_docs_ids = []\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    _id = doc_ids[i]  # 현재 문서의 ID를 가져옵니다.\n",
    "    # 현재 문서를 하위 문서로 분할합니다.\n",
    "    split_doc = text_splitter.split_documents([doc])\n",
    "    for _doc in split_doc:  # 분할된 문서에 대해 반복합니다.\n",
    "        # 문서의 메타데이터에 ID를 저장합니다.\n",
    "        _doc.metadata[id_key] = _id\n",
    "        split_docs_ids.append(_id)\n",
    "    split_docs.extend(split_doc)  # 분할된 문서를 리스트에 추가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4912dc49",
   "metadata": {},
   "source": [
    "분할된 문서의 개수를 출력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30ec2bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분할된 문서의 개수: 17\n"
     ]
    }
   ],
   "source": [
    "print(f\"분할된 문서의 개수: {len(split_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d769298d",
   "metadata": {},
   "source": [
    "마지막으로 분할된 문서를 벡터 저장소에 추가합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "107f38eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['160b366d-d723-4a1a-bc76-a85f0087bd21',\n",
       " 'bfa767db-3adf-4754-ba9c-8f9f179eb695',\n",
       " '754f006c-f4c0-4c51-9124-1c61742a4097',\n",
       " 'd25bf616-506f-4ecc-9450-fac5d85bb96f',\n",
       " 'ea96a0e1-f504-4c04-9349-178d29314416',\n",
       " '625c3310-0d63-48ff-b4e8-e3c58e073af5',\n",
       " 'b023fe71-9b6a-4426-9ce0-8c4f9c5b495f',\n",
       " 'e20e9190-a897-4b15-b292-df90e0469642',\n",
       " '47ebecc7-1fb5-48e5-89ff-77ec1e0ee24f',\n",
       " '480fbc27-9dec-4b86-830d-2af13a5a22ea',\n",
       " '0d2788b7-8621-4f93-a69e-7fc1045da5ea',\n",
       " '79152ff5-6e18-4eb8-bca7-21b8a1f720e0',\n",
       " 'ebe496b8-772e-4b78-8e88-20d5b0c9ea46',\n",
       " '33ebdb58-3534-43d7-bebb-e5b5a86e9194',\n",
       " '22e0d892-8af1-4cb6-aeb5-80f380c2c3eb',\n",
       " '12a76445-4031-4c5e-8519-01c42f6f1ccc',\n",
       " 'ab2bc54d-4538-4dbf-8179-2f1de3a0fdb0']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서를 검색기의 벡터 저장소에 추가합니다.\n",
    "retriever.vectorstore.add_documents(split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9158831",
   "metadata": {},
   "source": [
    "`vectorstore` 객체의 `similarity_search` 메서드를 사용하여 유사도 검색을 수행합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89e3d643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사도 검색을 수행합니다.\n",
    "result_docs = vectorstore.similarity_search(\"Word2Vec의 정의가 뭐야?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae7cb5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'doc_id': '43acf120-fcbc-48cb-bea6-503d1d5ab377', 'source': './data/ai-story.txt'}, page_content='Word2Vec의 성공 이후, 이와 유사한 다른 단어 임베딩 기법들도 개발되었습니다. 그러나 Word2Vec은 그 간결함과 효율성, 높은 성능으로 인해 여전히 광범위하게 사용되며, NLP 분야에서 기본적인 도구로 자리 잡았습니다. Word2Vec는 단순한 텍스트 데이터를 통해 복잡한 언어의 의미 구조를 학습할 수 있는 강력한 방법을 제공함으로써, 컴퓨터가 인간 언어를 이해하는 방식을 혁신적으로 개선하였습니다.')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1개의 결과 문서를 출력합니다.\n",
    "result_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd64a80",
   "metadata": {},
   "source": [
    "`retriever` 객체의 `invoke` 메서드를 사용하여 질문과 관련된 문서를 검색합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5a9c432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 관련된 문서를 검색하여 가져옵니다.\n",
    "retrieved_docs = retriever.invoke(\"Word2Vec 의 정의가 뭐야?\")\n",
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5ef9cd",
   "metadata": {},
   "source": [
    "`retrieved_docs[0].page_content`의 길이를 반환합니다. 문서를 반환하기 때문에 페이지 내용의 길이는 청크보다 일반적으로 큽니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce98a624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7482"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 검색된 문서의 첫 번째 문서의 페이지 내용의 길이를 반환합니다.\n",
    "len(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3d5f1b",
   "metadata": {},
   "source": [
    "## 가설 쿼리(Hypothetical Queries)\n",
    "\n",
    "LLM은 특정 문서에 대해 가정할 수 있는 질문 목록을 생성하는 데에도 사용될 수 있습니다.\n",
    "\n",
    "이렇게 생성된 질문들은 임베딩(embedding)될 수 있으며, 이를 통해 문서의 내용을 더욱 깊이 있게 탐색하고 이해할 수 있습니다.\n",
    "\n",
    "가정 질문 생성은 문서의 주요 주제와 개념을 파악하는 데 도움이 되며, 독자들이 문서 내용에 대해 더 많은 궁금증을 갖도록 유도할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7933e03",
   "metadata": {},
   "source": [
    "- `functions` 리스트에는 하나의 딕셔너리 요소가 포함되어 있습니다.\n",
    "- 딕셔너리는 `name`, `description`, `parameters` 키를 가지고 있습니다.\n",
    "- `name`은 함수의 이름을 나타내는 문자열입니다.\n",
    "- `description`은 함수의 설명을 나타내는 문자열입니다.\n",
    "- `parameters`는 함수의 매개변수를 정의하는 딕셔너리입니다.\n",
    "  - `type`은 매개변수의 타입을 나타내며, 여기서는 \"object\"로 설정되어 있습니다.\n",
    "  - `properties`는 객체의 속성을 정의하는 딕셔너리입니다.\n",
    "    - `questions`는 \"array\" 타입의 속성으로, 각 요소는 \"string\" 타입입니다.\n",
    "  - `required`는 필수 속성을 나타내는 리스트이며, 여기서는 `questions`가 필수로 지정되어 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5baf8ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\": \"hypothetical_questions\",  # 함수의 이름을 지정합니다.\n",
    "        \"description\": \"Generate hypothetical questions\",  # 함수에 대한 설명을 작성합니다.\n",
    "        \"parameters\": {  # 함수의 매개변수를 정의합니다.\n",
    "            \"type\": \"object\",  # 매개변수의 타입을 객체로 지정합니다.\n",
    "            \"properties\": {  # 객체의 속성을 정의합니다.\n",
    "                \"questions\": {  # 'questions' 속성을 정의합니다.\n",
    "                    \"type\": \"array\",  # 'questions'의 타입을 배열로 지정합니다.\n",
    "                    \"items\": {\n",
    "                        \"type\": \"string\"\n",
    "                    },  # 배열의 요소 타입을 문자열로 지정합니다.\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"questions\"],  # 필수 매개변수로 'questions'를 지정합니다.\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aa557a",
   "metadata": {},
   "source": [
    "`ChatPromptTemplate`을 사용하여 주어진 문서를 기반으로 3개의 가상 질문을 생성하는 프롬프트 템플릿을 정의합니다.\n",
    "\n",
    "- `ChatOpenAI`를 사용하여 GPT 모델을 초기화하고, `functions`와 `function_call`을 설정하여 가상 질문 생성 함수를 호출합니다.\n",
    "- `JsonKeyOutputFunctionsParser`를 사용하여 생성된 가상 질문을 파싱하고, `questions` 키에 해당하는 값을 추출합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7cb4be57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n",
    "\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    # 아래 문서를 사용하여 답변할 수 있는 가상의 질문을 정확히 3개 생성하도록 요청합니다. 이 숫자는 조정될 수 있습니다.\n",
    "    | ChatPromptTemplate.from_template(\n",
    "        \"Generate a list of exactly 3 hypothetical questions that the below document could be used to answer. Answer in Korean:\\n\\n{doc}\"\n",
    "    )\n",
    "    | ChatOpenAI(max_retries=0, model=\"gpt-4o\").bind(\n",
    "        functions=functions, function_call={\"name\": \"hypothetical_questions\"}\n",
    "    )\n",
    "    # 출력에서 \"questions\" 키에 해당하는 값을 추출합니다.\n",
    "    | JsonKeyOutputFunctionsParser(key_name=\"questions\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b789211",
   "metadata": {},
   "source": [
    "`chain.invoke(docs[0])` 호출하여 첫 번째 문서에 대한 답변을 출력합니다.\n",
    "\n",
    "- 출력은 생성한 3개의 가설 쿼리(Hypothetical Queries) 가 담겨 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a9cda3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scikit-learn을 사용하여 어떤 종류의 기계 학습 모델을 개발할 수 있습니까?',\n",
       " 'NLP 기술은 어떤 산업 분야에서 활용될 수 있습니까?',\n",
       " 'SciPy 라이브러리를 사용하여 어떤 종류의 과학적 계산을 수행할 수 있습니까?']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 주어진 문서에 대해 체인을 실행합니다.\n",
    "chain.invoke(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853ff912",
   "metadata": {},
   "source": [
    "`chain.batch` 메서드를 사용하여 `docs` 데이터에 대해 동시에 여러 개의 요청을 처리합니다.\n",
    "\n",
    "- `docs` 매개변수는 처리할 문서 데이터를 나타냅니다.\n",
    "- `max_concurrency` 매개변수는 동시에 처리할 수 있는 최대 요청 수를 지정합니다. 이 예시에서는 5로 설정되어 있습니다.\n",
    "- 이 메서드는 `docs` 데이터의 각 항목에 대해 `chain` 객체의 작업을 수행하고, 최대 5개의 요청을 동시에 처리합니다.\n",
    "- 처리 결과는 `hypothetical_questions` 변수에 저장됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac642190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 목록에 대해 가설적 질문을 일괄 처리하여 생성합니다. 최대 동시성은 5로 설정되어 있습니다.\n",
    "hypothetical_questions = chain.batch(docs, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02840bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Scikit-learn을 사용하여 어떤 유형의 기계 학습 문제를 해결할 수 있나요?', 'Word2Vec 모델을 통해 어떤 방식으로 단어의 의미적 유사성을 분석할 수 있나요?', 'SciPy와 NumPy를 결합하여 어떤 과학 계산 문제를 해결할 수 있나요?']\n",
      "['의미론적 검색의 원리는 무엇이며 어떻게 작동하는가?', '자연어 처리 기술 중 임베딩은 어떤 역할을 하며 어떻게 사용되는가?', '텍스트 데이터를 분석할 때 토큰화와 토크나이저의 중요성은 무엇인가?']\n"
     ]
    }
   ],
   "source": [
    "print(hypothetical_questions[0])\n",
    "print(hypothetical_questions[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f364ded0",
   "metadata": {},
   "source": [
    "아래는 이전에 진행했던 방식과 동일하게 생성한 가설 쿼리(Hypothetical Queries) 를 벡터저장소에 저장하는 과정입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c44404c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자식 청크를 인덱싱하는 데 사용할 벡터 저장소\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"hypo-questions\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "# 부모 문서의 저장소 계층\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "# 검색기 (시작 시 비어 있음)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]  # 문서 ID 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f8a6b7",
   "metadata": {},
   "source": [
    "`question_docs` 리스트에 메타데이터(문서 ID) 를 추가합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7cbb65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_docs = []\n",
    "# hypothetical_questions 리스트를 순회하면서 인덱스와 질문 리스트를 가져옵니다.\n",
    "for i, question_list in enumerate(hypothetical_questions):\n",
    "    question_docs.extend(  # question_docs 리스트에 Document 객체를 추가합니다.\n",
    "        # 질문 리스트의 각 질문에 대해 Document 객체를 생성하고, 메타데이터에 해당 질문의 문서 ID를 포함시킵니다.\n",
    "        [Document(page_content=s, metadata={\n",
    "                  id_key: doc_ids[i]}) for s in question_list]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480a8e5f",
   "metadata": {},
   "source": [
    "가설 쿼리를 문서에 추가하고, 원본 문서를 `docstore` 에 추가합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "105ecf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.vectorstore.add_documents(\n",
    "    question_docs\n",
    ")  # 질문 문서를 벡터 저장소에 추가합니다.\n",
    "# 문서 ID와 문서를 매핑하여 문서 저장소에 저장합니다.\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badb5835",
   "metadata": {},
   "source": [
    "`vectorstore` 객체의 `similarity_search` 메서드를 사용하여 유사도 검색을 수행합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca9afb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사한 문서를 벡터 저장소에서 검색합니다.\n",
    "result_docs = vectorstore.similarity_search(\"Word2Vec에 대한 정의는 뭐야?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ce41e6",
   "metadata": {},
   "source": [
    "아래는 유사도 검색 결과입니다.\n",
    "\n",
    "여기서는 생성한 가설 쿼리만 추가해 놓은 상태이기 때문에, 생성한 가설 쿼리 중 유사도가 가장 높은 문서를 반환합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10c33bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'doc_id': '81dc1ff6-4216-4fb1-aeea-001149b7077f'}, page_content='Word2Vec 모델을 통해 어떤 방식으로 단어의 의미적 유사성을 분석할 수 있나요?'),\n",
       " Document(metadata={'doc_id': '023a3082-894d-47ad-8a2f-77eb8180e794'}, page_content='의미론적 검색의 원리는 무엇이며 어떻게 작동하는가?'),\n",
       " Document(metadata={'doc_id': '023a3082-894d-47ad-8a2f-77eb8180e794'}, page_content='텍스트 데이터를 분석할 때 토큰화와 토크나이저의 중요성은 무엇인가?'),\n",
       " Document(metadata={'doc_id': '023a3082-894d-47ad-8a2f-77eb8180e794'}, page_content='자연어 처리 기술 중 임베딩은 어떤 역할을 하며 어떻게 사용되는가?')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 유사도 검색 결과를 출력합니다.\n",
    "result_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fbb03d",
   "metadata": {},
   "source": [
    "이전 단계에서 분할한 문서도 벡터저장소에 추가합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6c84315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['97da29ba-da86-4871-9087-b30934a70290',\n",
       " '1db2e98b-01db-4418-9971-f02918ddcf4c',\n",
       " '4143971b-d15d-46bc-a4ac-15514a182747',\n",
       " 'e86a8640-3cc4-4b0d-a8a1-ab1124342a81',\n",
       " '85a2f534-97a1-4e3f-9e51-68d92ec63e3b',\n",
       " 'bfca23ea-0a5d-44bf-b541-db6d71ee2798',\n",
       " 'e561e718-fe93-4256-8adf-c19dedc44f96',\n",
       " '94af400b-0fb7-479b-93ad-c4246dba415f',\n",
       " 'e60bc999-2902-4587-a074-db4bd75e737f',\n",
       " '1db9d3b8-4807-498f-9a37-f9c6d886d8f4',\n",
       " 'e26416c8-ccf0-4e2c-8371-8f2c0765ddc8',\n",
       " '9b425433-71f7-47f8-a077-ac79b7288a88',\n",
       " '1b2998af-f1bb-447f-9ed8-056e566b0af2',\n",
       " 'a9600cc4-6905-4687-8c06-d3d9ca430ff1',\n",
       " '2ef710e5-d2ca-4455-9bc3-6af19db9a97f',\n",
       " 'dd611cc0-7807-440a-9b77-ec96f31b6ad6',\n",
       " '8a21a037-c46c-434e-a973-33dd3fe45aec']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서를 검색기의 벡터 저장소에 추가합니다.\n",
    "retriever.vectorstore.add_documents(split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88722c7",
   "metadata": {},
   "source": [
    "`retriever` 객체의 `invoke` 메서드를 사용하여 쿼리와 관련된 문서를 검색합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d106a8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 관련된 문서를 검색하여 가져옵니다.\n",
    "retrieved_docs = retriever.invoke(\"Word2Vec에 대한 정의가 뭐야?\")\n",
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a4bf86",
   "metadata": {},
   "source": [
    "검색 결과인 `retrieved_docs[0].page_content`의 길이를 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "74403220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7482"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "# 검색된 문서의 첫 번째 문서의 페이지 내용의 길이를 반환합니다.\n",
    "len(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dd199a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
