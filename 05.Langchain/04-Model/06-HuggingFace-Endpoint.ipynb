{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f663c69",
   "metadata": {},
   "source": [
    "# Huggingface Endpoints\n",
    "\n",
    "Hugging Face Hub은 12만 개 이상의 모델, 2만 개의 데이터셋, 5만 개의 데모 앱(Spaces)을 보유한 플랫폼으로, 모두 오픈 소스이며 공개적으로 사용 가능합니다. 이 온라인 플랫폼에서 사람들은 쉽게 협업하고 함께 머신러닝을 구축할 수 있습니다.\n",
    "\n",
    "Hugging Face Hub은 또한 다양한 ML 애플리케이션을 구축하기 위한 다양한 엔드포인트를 제공합니다. 이 예제는 다양한 유형의 엔드포인트에 연결하는 방법을 보여줍니다.\n",
    "\n",
    "특히, 텍스트 생성 추론은 Text Generation Inference에 의해 구동됩니다. 이는 매우 빠른 텍스트 생성 추론을 위해 맞춤 제작된 Rust, Python, gRPC 서버입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7a0967",
   "metadata": {},
   "source": [
    "## 허깅페이스 토큰 발급\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5addfb",
   "metadata": {},
   "source": [
    "허깅페이스(https://huggingface.co) 에 회원가입을 한 뒤, 아래의 주소에서 토큰 발급을 신청합니다.\n",
    "\n",
    "- 토큰 발급주소: https://huggingface.co/docs/hub/security-tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ede30ac",
   "metadata": {},
   "source": [
    "## HuggingFace 모델 리스트\n",
    "\n",
    "- 허깅페이스 LLM 리더보드: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n",
    "- 모델 리스트: https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf342258",
   "metadata": {},
   "source": [
    "## Installation and Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0071922f",
   "metadata": {},
   "source": [
    "`HuggingFaceEndpoint` 클래스를 사용하여 Hugging Face 엔드포인트와 상호 작용할 수 있습니다.\n",
    "\n",
    "- `langchain_community.llms` 모듈에서 `HuggingFaceEndpoint` 클래스를 임포트합니다.\n",
    "- `HuggingFaceEndpoint` 클래스를 사용하면 Hugging Face에서 호스팅되는 언어 모델과 통신할 수 있습니다.\n",
    "- 이 클래스는 Hugging Face 엔드포인트의 URL과 필요한 인증 정보를 사용하여 초기화됩니다.\n",
    "- 초기화된 `HuggingFaceEndpoint` 객체를 통해 프롬프트를 전송하고 언어 모델의 응답을 받을 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9649a9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18380b00",
   "metadata": {},
   "source": [
    "사용하기 위해서는 Python의 `huggingface_hub` [패키지를 설치](https://huggingface.co/docs/huggingface_hub/installation)해야 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfe42a1",
   "metadata": {},
   "source": [
    "- `huggingface_hub` 라이브러리를 최신 버전으로 업그레이드합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed667993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793c830b",
   "metadata": {},
   "source": [
    "아래의 코드를 실행하여 발급 받은 허깅페이스 토큰을 인증합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a99d653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544c92ed873942b0af278b2866981158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039193c3",
   "metadata": {},
   "source": [
    "이 코드는 Hugging Face Hub API 토큰을 안전하게 입력받아 `HUGGINGFACEHUB_API_TOKEN` 변수에 저장하는 역할을 합니다. 토큰은 Hugging Face Hub의 API 추론 기능을 사용하기 위해 필요합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6912460",
   "metadata": {},
   "source": [
    "- 환경 변수 `HUGGINGFACEHUB_API_TOKEN`을 설정합니다.\n",
    "  - `os.environ` 딕셔너리를 사용하여 `\"HUGGINGFACEHUB_API_TOKEN\"` 키에 `HUGGINGFACEHUB_API_TOKEN` 변수의 값을 할당합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24628822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93526f68",
   "metadata": {},
   "source": [
    "## Prepare Examples\n",
    "\n",
    "예시 데이터를 준비하는 과정입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65fdd98",
   "metadata": {},
   "source": [
    "`HuggingFaceEndpoint` 클래스를 사용하여 Hugging Face 엔드포인트와 상호 작용할 수 있습니다.\n",
    "\n",
    "- `langchain_community.llms` 모듈에서 `HuggingFaceEndpoint` 클래스를 임포트합니다.\n",
    "- `HuggingFaceEndpoint` 클래스를 사용하면 Hugging Face에서 호스팅되는 언어 모델과 통신할 수 있습니다.\n",
    "- 이 클래스는 Hugging Face 엔드포인트의 URL과 필요한 인증 정보를 사용하여 초기화됩니다.\n",
    "- 초기화된 `HuggingFaceEndpoint` 객체를 통해 프롬프트를 전송하고 언어 모델의 응답을 받을 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f7661c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4c7da8",
   "metadata": {},
   "source": [
    "- `question` 변수에 \"1994년 FIFA 월드컵에서 우승한 팀은 어디인가요?\"라는 질문을 할당합니다.\n",
    "- `template` 변수에 질문과 답변 형식을 지정하는 템플릿 문자열을 할당합니다.\n",
    "  - 템플릿에는 `{question}` 플레이스홀더가 포함되어 있습니다.\n",
    "  - 답변 부분에는 \"단계별로 생각해 보겠습니다.\"라는 문구가 포함되어 있습니다.\n",
    "- `PromptTemplate.from_template()` 메서드를 사용하여 `template`을 기반으로 `PromptTemplate` 객체인 `prompt`를 생성합니다.\n",
    "  - 이를 통해 질문을 템플릿에 삽입하여 프롬프트를 생성할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01e4a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Please answer the following questions concisely.\n",
    "QUESTION: {question}\n",
    "\n",
    "ANSWER: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4fdc3b",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "다음은 무료 [Serverless Endpoints](https://huggingface.co/docs/api-inference/index) API의 `HuggingFaceEndpoint` 통합에 액세스하는 방법의 예시입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad3ae86",
   "metadata": {},
   "source": [
    "- `repo_id` 변수에 \"mistralai/Mistral-7B-Instruct-v0.2\" 모델의 저장소 ID를 할당합니다.\n",
    "- `HuggingFaceEndpoint`를 사용하여 `llm` 객체를 생성합니다.\n",
    "  - `repo_id`로 지정된 모델을 사용합니다.\n",
    "  - `max_length`를 128로 설정하여 생성할 최대 토큰 수를 제한합니다.\n",
    "  - `temperature`를 0.5로 설정하여 생성 결과의 다양성을 조절합니다.\n",
    "  - `token`에 `HUGGINGFACEHUB_API_TOKEN`을 전달하여 인증합니다.\n",
    "- `LLMChain`을 사용하여 `llm_chain` 객체를 생성합니다.\n",
    "  - `prompt` 변수에 할당된 프롬프트를 사용합니다.\n",
    "  - `llm` 매개변수에 이전에 생성한 `llm` 객체를 전달합니다.\n",
    "- `llm_chain.run(question)`을 호출하여 주어진 질문에 대한 답변을 생성합니다.\n",
    "- 생성된 답변을 출력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b5a7b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\donchang\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEndpoint` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEndpoint`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\donchang\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\donchang\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Eiffel Tower: An iconic symbol of Paris and a must-visit attraction.\n",
      "2. Louvre Museum: Home to thousands of works of art, including the Mona Lisa.\n",
      "3. Notre-Dame Cathedral: A historic and architecturally stunning cathedral.\n",
      "4. Montmartre: A bohemian district known for its artistic history and beautiful views of Paris.\n",
      "5. Champs-Élysées: A famous avenue lined with cafes, shops, and the Arc de Triomphe.</s>{'question': 'Please tell me top 5 places to visit in Paris.', 'text': '1. Eiffel Tower: An iconic symbol of Paris and a must-visit attraction.\\n2. Louvre Museum: Home to thousands of works of art, including the Mona Lisa.\\n3. Notre-Dame Cathedral: A historic and architecturally stunning cathedral.\\n4. Montmartre: A bohemian district known for its artistic history and beautiful views of Paris.\\n5. Champs-Élysées: A famous avenue lined with cafes, shops, and the Arc de Triomphe.</s>'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# 사용할 모델의 저장소 ID를 설정합니다.\n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "#repo_id = \"google/gemma-2-9b\"\n",
    "\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,  # 모델 저장소 ID를 지정합니다.\n",
    "    max_new_tokens=256,  # 생성할 최대 토큰 길이를 설정합니다.\n",
    "    temperature=0.1,  # 샘플링 온도를 설정합니다. 값이 높을수록 더 다양한 출력을 생성합니다.\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],  # 콜백을 설정합니다.\n",
    "    streaming=True,  # 스트리밍을 사용합니다.\n",
    ")\n",
    "\n",
    "# LLMChain을 초기화하고 프롬프트와 언어 모델을 전달합니다.\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "# 질문을 전달하여 LLMChain을 실행하고 결과를 출력합니다.\n",
    "response = llm_chain.invoke(\n",
    "    {\"question\": \"Please tell me top 5 places to visit in Paris.\"}\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71daf58c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "410ba3d7",
   "metadata": {},
   "source": [
    "## Dedicated Endpoint\n",
    "\n",
    "무료 서버리스 API를 사용하면 솔루션을 빠르게 구현하고 반복할 수 있지만, 로드가 다른 요청과 공유되기 때문에 대용량 사용 사례에서는 속도 제한이 있을 수 있습니다.\n",
    "\n",
    "엔터프라이즈 워크로드의 경우, [Inference Endpoints - Dedicated](https://huggingface.co/inference-endpoints/dedicated)를 사용하는 것이 가장 좋습니다.\n",
    "\n",
    "이를 통해 더 많은 유연성과 속도를 제공하는 완전 관리형 인프라에 액세스할 수 있습니다.\n",
    "\n",
    "이러한 리소스에는 지속적인 지원과 가동 시간 보장은 물론 AutoScaling과 같은 옵션도 포함됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118bf744",
   "metadata": {},
   "source": [
    "![Huggingface dedicated endpoint](e:\\\\course\\\\data\\\\huggingface.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5b0969",
   "metadata": {},
   "source": [
    "- `your_endpoint_url` 변수에 Inference Endpoint의 URL을 설정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26f30fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Endpoint URL을 아래에 설정합니다.\n",
    "your_endpoint_url = \"https://qkryokt2o80cnb8u.us-east-1.aws.endpoints.huggingface.cloud\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585526bd",
   "metadata": {},
   "source": [
    "- `HuggingFaceEndpoint` 클래스를 사용하여 `llm` 객체를 생성합니다.\n",
    "  - `endpoint_url` 매개변수에는 Hugging Face 엔드포인트의 URL을 전달합니다.\n",
    "  - `max_new_tokens` 매개변수는 생성할 최대 토큰 수를 지정합니다.\n",
    "  - `top_k` 매개변수는 확률 기반 샘플링에 사용할 상위 k개 토큰을 지정합니다.\n",
    "  - `top_p` 매개변수는 누적 확률 기반 샘플링에 사용할 상위 확률 임계값을 지정합니다.\n",
    "  - `typical_p` 매개변수는 전형적인 확률 기반 샘플링에 사용할 확률 임계값을 지정합니다.\n",
    "  - `temperature` 매개변수는 샘플링 과정에서의 무작위성을 조절합니다.\n",
    "  - `repetition_penalty` 매개변수는 반복 패널티를 조절하여 반복되는 단어나 구문을 방지합니다.\n",
    "- `llm` 객체를 호출하여 \"#QUESTION: 대한민국의 수도는 어디인가요?\\n\\n#ANSWER:\" 라는 질문을 전달하면, 해당 질문에 대한 답변을 생성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14272123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\donchang\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "404 Client Error: Not Found for url: https://qkryokt2o80cnb8u.us-east-1.aws.endpoints.huggingface.cloud/ (Request ID: CRbhzh)\n\nNot Found: qkryokt2o80cnb8u.us-east-1.aws.endpoints.huggingface.cloud",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\donchang\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\donchang\\anaconda3\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://qkryokt2o80cnb8u.us-east-1.aws.endpoints.huggingface.cloud/",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 18\u001b[0m\n\u001b[0;32m      1\u001b[0m llm \u001b[38;5;241m=\u001b[39m HuggingFaceEndpoint(\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# 엔드포인트 URL을 설정합니다.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     endpoint_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myour_endpoint_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     repetition_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.03\u001b[39m,\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 주어진 프롬프트에 대해 언어 모델을 실행합니다.\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m#QUESTION: 대한민국의 수도는 어디인가요?\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m#ANSWER:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\donchang\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:346\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    343\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    344\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 346\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    356\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    357\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    358\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\donchang\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:703\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    697\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    701\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    702\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 703\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\donchang\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:882\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    868\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    869\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    870\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    880\u001b[0m         )\n\u001b[0;32m    881\u001b[0m     ]\n\u001b[1;32m--> 882\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\donchang\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:740\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    739\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 740\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    741\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32mc:\\Users\\donchang\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:727\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    718\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    719\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    723\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    724\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    725\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    726\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 727\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    729\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    730\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[0;32m    731\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    732\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    735\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    736\u001b[0m         )\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    738\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\donchang\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1431\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1428\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1430\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1431\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1432\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1433\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1434\u001b[0m     )\n\u001b[0;32m   1435\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32mc:\\Users\\donchang\\anaconda3\\Lib\\site-packages\\langchain_community\\llms\\huggingface_endpoint.py:262\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    259\u001b[0m     invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m invocation_params[\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    261\u001b[0m     ]  \u001b[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001b[39;00m\n\u001b[1;32m--> 262\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    268\u001b[0m         response_text \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\donchang\\anaconda3\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:273\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 273\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\Users\\donchang\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:371\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 371\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://qkryokt2o80cnb8u.us-east-1.aws.endpoints.huggingface.cloud/ (Request ID: CRbhzh)\n\nNot Found: qkryokt2o80cnb8u.us-east-1.aws.endpoints.huggingface.cloud"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    # 엔드포인트 URL을 설정합니다.\n",
    "    endpoint_url=f\"{your_endpoint_url}\",\n",
    "    # 생성할 최대 토큰 수를 설정합니다.\n",
    "    max_new_tokens=512,\n",
    "    # 상위 K개의 토큰을 선택합니다.\n",
    "    top_k=10,\n",
    "    # 누적 확률이 top_p에 도달할 때까지 토큰을 선택합니다.\n",
    "    top_p=0.95,\n",
    "    # typical_p 확률 이상의 토큰만 선택합니다.\n",
    "    typical_p=0.95,\n",
    "    # 샘플링 온도를 설정합니다. 낮을수록 더 결정적입니다.\n",
    "    temperature=0.01,\n",
    "    # 반복 패널티를 설정합니다. 높을수록 반복을 줄입니다.\n",
    "    repetition_penalty=1.03,\n",
    ")\n",
    "# 주어진 프롬프트에 대해 언어 모델을 실행합니다.\n",
    "llm.invoke(input=\"#QUESTION: 대한민국의 수도는 어디인가요?\\n\\n#ANSWER:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334491dc",
   "metadata": {},
   "source": [
    "HuggingFaceEndpoint를 사용하여 스트리밍 방식으로 텍스트를 생성하는 예제입니다.\n",
    "\n",
    "- `HuggingFaceEndpoint` 클래스를 사용하여 `llm` 객체를 생성합니다.\n",
    "  - `endpoint_url` 매개변수에 HuggingFace 엔드포인트 URL을 지정합니다.\n",
    "  - 다양한 생성 매개변수를 설정합니다: `max_new_tokens`, `top_k`, `top_p`, `typical_p`, `temperature`, `repetition_penalty`.\n",
    "  - `streaming` 매개변수를 `True`로 설정하여 스트리밍 모드를 활성화합니다.\n",
    "- `StreamingStdOutCallbackHandler`를 콜백으로 사용하여 생성된 텍스트를 실시간으로 출력합니다.\n",
    "- `llm` 객체를 호출하여 프롬프트에 대한 텍스트를 생성합니다.\n",
    "\n",
    "이 예제는 HuggingFace 엔드포인트를 사용하여 텍스트 생성을 수행하고, 생성 과정을 실시간으로 스트리밍하는 방법을 보여줍니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd9c93c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 안녕하세요? 저는 한국관광공사에서 운영하는 트래블리더로 활동하고 있는 대학생입니다! 제가 소개시켜드릴 관광지는 바로 수원화성과 화성행궁, 그리고 광교호수공원인데요~ 지금부터 그 이유와 함께 설명해 드리도록 하겠습니다^^ 첫째로 들려볼 장소는 세계문화유산으로 등록되어있는 \\'수원화성\\' 입니다!! 우리나라의 성곽 중 가장 아름답다고 평가받고 있기 때문에 많은 분들이 찾아주십니다~~ 특히 야경이 정말 예쁘다는 사실~! 두번째 코스는 조선왕조 최대규모 행차였던 어머니에 대한 효심을 담은 왕의 길인 \"화성어장\" 을 걸으며 역사적 의미도 느껴보며 산책하기 좋은곳 인것 같네ㅎㅎ 마지막 세 번째코스는 호수위 공중정원 으로 불리는 ‘광교호수공원’ 입다!!! 자연친화적인 휴식처이며 다양한 문화예술프로그램들을 접하거나 참여 할 수 있어서 더욱 매력만점이고 사람들의 발길이 계속해서 향하게 되는 것같습니당^0^* 이상 간단하지만 알찬 정보들 잘 보셨나용?? 여러분께 도움되는 포스트였다면 좋겠구 앞으로 열심히 노력하여 유익한 글 많이 올리도록 약속하겟슴돠~~~ 감사합니다♥<eos>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    # 엔드포인트 URL을 설정합니다.\n",
    "    endpoint_url=f\"{your_endpoint_url}\",\n",
    "    # 생성할 최대 토큰 수를 설정합니다.\n",
    "    max_new_tokens=512,\n",
    "    # top_k 샘플링 기법에서 고려할 최상위 토큰 수를 설정합니다.\n",
    "    top_k=10,\n",
    "    # top_p 샘플링 기법에서 누적 확률 임계값을 설정합니다.\n",
    "    top_p=0.95,\n",
    "    # typical_p 샘플링 기법에서 누적 확률 임계값을 설정합니다.\n",
    "    typical_p=0.95,\n",
    "    # 샘플링 온도를 설정합니다. 낮을수록 더 결정적인 출력을 생성합니다.\n",
    "    temperature=0.01,\n",
    "    # 반복 패널티를 설정하여 동일한 토큰의 반복을 제어합니다.\n",
    "    repetition_penalty=1.03,\n",
    "    # 스트리밍 모드를 활성화합니다.\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    ")\n",
    "\n",
    "# 질문을 입력하고 스트리밍 콜백 핸들러를 사용하여 실시간으로 출력을 확인합니다.\n",
    "llm.invoke(\n",
    "    input=\"#QUESTION: 대한민국 경기도를 여행한다면 꼭 가봐야할 곳 5군데를 추천해 주세요.n\\n#ANSWER:\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b01bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"bartowski/gemma-2-27b-it-GGUF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548d81bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"bartowski/gemma-2-27b-it-GGUF\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
