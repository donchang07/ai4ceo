{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í† í° ì •ë³´ë¡œë“œë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "# ì„¤ì¹˜: pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# í† í° ì •ë³´ë¡œë“œ\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ì˜¤ë””ì˜¤íŒŒì¼ì— ìë§‰ìƒì„±\n",
    "- response_format=\"srt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = open(\"data/ì±„ìš©ë©´ì ‘_ìƒ˜í”Œ_01.wav\", \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    file=audio_file,\n",
    "    model=\"whisper-1\",\n",
    "    language=\"ko\",\n",
    "    response_format=\"srt\",  # ìë§‰ í¬ë§·\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. YouTube urlì—ì„œ ìŒì„±íŒŒì¼ì„ ë§Œë“¤ê¸°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "import os\n",
    "\n",
    "link = \"https://www.youtube.com/watch?v=ZsQX7x_KWjo&t=2851s\"\n",
    "\n",
    "yt = YouTube(link)\n",
    "filename = yt.streams.filter(only_audio=True).first().download()\n",
    "renamed_file = filename.replace(\".mp4\", \".mp3\")\n",
    "os.rename(filename, renamed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(renamed_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 MP3ë¥¼  WAV ë³€í™˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import AudioFileClip\n",
    "\n",
    "\n",
    "def convert_mp3_to_wav(filepath):\n",
    "    # WAV íŒŒì¼ ê²½ë¡œ\n",
    "    wav_file_path = filepath.replace(\".mp3\", \".wav\")\n",
    "\n",
    "    # MP4 íŒŒì¼ ë¡œë“œ\n",
    "    audio_clip = AudioFileClip(filepath)\n",
    "\n",
    "    # WAV í˜•ì‹ìœ¼ë¡œ ì˜¤ë””ì˜¤ ì¶”ì¶œ ë° ì €ì¥\n",
    "    audio_clip.write_audiofile(wav_file_path, fps=44100, nbytes=2, codec=\"pcm_s16le\")\n",
    "    return wav_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_file = convert_mp3_to_wav(renamed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# ì˜¤ë””ì˜¤ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "audio = AudioSegment.from_file(wav_file, format=\"wav\")\n",
    "\n",
    "total_length = len(audio)\n",
    "length_per_chunk = 60 * 1000  # 60ì´ˆ\n",
    "\n",
    "if not os.path.exists(\".tmp\"):\n",
    "    os.mkdir(\".tmp\")\n",
    "\n",
    "folder_path = os.path.join(\".tmp\", wav_file[:-4])\n",
    "\n",
    "if not os.path.exists(folder_path):\n",
    "    os.mkdir(folder_path)\n",
    "\n",
    "chunks = []\n",
    "for i in range(0, total_length, length_per_chunk):\n",
    "    chunk_file_path = os.path.join(folder_path, f\"{i}.wav\")\n",
    "    audio[i : i + length_per_chunk].export(chunk_file_path, format=\"wav\")\n",
    "    chunks.append(chunk_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "\n",
    "def adjust_timestamps(transcript, minutes=1):\n",
    "    # Define the regular expression pattern for timestamps\n",
    "    timestamp_pattern = re.compile(r\"(\\d{2}:\\d{2}:\\d{2},\\d{3})\")\n",
    "\n",
    "    # Function to add minutes to a timestamp\n",
    "    def add_minutes(timestamp_str, minutes):\n",
    "        timestamp = datetime.strptime(timestamp_str, \"%H:%M:%S,%f\")\n",
    "        adjusted_timestamp = timestamp + timedelta(minutes=minutes)\n",
    "        return adjusted_timestamp.strftime(\"%H:%M:%S,%f\")[:-3]\n",
    "\n",
    "    # Replace timestamps in the transcript\n",
    "    adjusted_transcript = timestamp_pattern.sub(\n",
    "        lambda match: add_minutes(match.group(1), minutes), transcript\n",
    "    )\n",
    "    return adjusted_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(chunk)\n",
    "    audio_file = open(chunk, \"rb\")\n",
    "    transcript = client.audio.transcriptions.create(\n",
    "        file=audio_file,\n",
    "        model=\"whisper-1\",\n",
    "        language=\"ko\",\n",
    "        response_format=\"srt\",\n",
    "        temperature=0.01,\n",
    "    )\n",
    "    transcripts.append(adjust_timestamps(transcript, minutes=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_transcripts(*transcripts):\n",
    "    merged_transcript = \"\"\n",
    "    current_number = 1\n",
    "\n",
    "    for transcript in transcripts:\n",
    "        # Split the transcript into segments\n",
    "        segments = transcript.strip().split(\"\\n\\n\")\n",
    "        for segment in segments:\n",
    "            # Split each segment into lines\n",
    "            lines = segment.split(\"\\n\")\n",
    "            # Replace the number at the beginning of each segment with the correct sequence number\n",
    "            lines[0] = str(current_number)\n",
    "            # Increment the sequence number\n",
    "            current_number += 1\n",
    "            # Reassemble the segment\n",
    "            merged_transcript += \"\\n\".join(lines) + \"\\n\\n\"\n",
    "\n",
    "    return merged_transcript.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merge_transcripts(*transcripts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_transcript = merge_transcripts(*transcripts)\n",
    "print(merged_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sample.srt\", \"w\") as f:\n",
    "    f.write(merged_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "import re\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "MAX_COMPLETION_TOKENS = 4096\n",
    "MAX_CONTEXT_LENGTH = 8192  # GPT-4ì˜ ì¼ë°˜ì ì¸ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def chunk_text(text: str, max_chunk_tokens: int = 3000) -> list[str]:\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_tokens = 0\n",
    "    \n",
    "    for sentence in text.split(\". \"):\n",
    "        sentence_tokens = count_tokens(sentence)\n",
    "        if current_tokens + sentence_tokens > max_chunk_tokens:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "            current_tokens = sentence_tokens\n",
    "        else:\n",
    "            current_chunk += sentence + \". \"\n",
    "            current_tokens += sentence_tokens\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def process_chunk(chunk: str, system_prompt: str) -> str:\n",
    "    prompt_tokens = count_tokens(system_prompt) + count_tokens(chunk)\n",
    "    max_response_tokens = min(MAX_COMPLETION_TOKENS, MAX_CONTEXT_LENGTH - prompt_tokens - 100)\n",
    "\n",
    "    if max_response_tokens <= 0:\n",
    "        return \"Error: ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤.\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=0.1,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": chunk},\n",
    "            ],\n",
    "            max_tokens=max_response_tokens,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {str(e)}\"\n",
    "\n",
    "def format_summary(summary: str) -> str:\n",
    "    lines = summary.split('\\n')\n",
    "    formatted_lines = []\n",
    "    for line in lines:\n",
    "        if line.strip():\n",
    "            match = re.match(r'^(\\d{2}:\\d{2})\\s+(.+)$', line)\n",
    "            if match:\n",
    "                time, content = match.groups()\n",
    "                if not any(emoji in content for emoji in ['ğŸ”¶', 'ğŸ‘‹', 'ğŸ‘€', 'ğŸ’¡', 'ğŸ¯', 'ğŸ“Š', 'ğŸ”‘', 'ğŸ’»', 'ğŸ¤”', 'ğŸ“']):\n",
    "                    content = 'ğŸ”¶ ' + content\n",
    "                formatted_lines.append(f\"{time} {content}\")\n",
    "            else:\n",
    "                formatted_lines.append(line)\n",
    "    return \"\\n\".join(formatted_lines)\n",
    "\n",
    "def post_processing(instruction: str) -> str:\n",
    "    system_prompt = \"\"\"\n",
    "    ë‹¹ì‹ ì€ ë¹„ë””ì˜¤ ìë§‰ì„ ë¶„ì„í•˜ê³  ìš”ì•½í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ìë§‰ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤:\n",
    "    1. ì¤‘ìš”í•œ ì£¼ì œë¥¼ ì‹œê°„ ìˆœì„œì— ë§ê²Œ ì„ ì •í•˜ì„¸ìš”.\n",
    "    2. ê° ì£¼ì œì— ëŒ€í•´ í•œ ì¤„ ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”.\n",
    "    3. ê° ì£¼ì œê°€ ì‹œì‘ë˜ëŠ” ì‹œê°„ì„ mm:ss(ë¶„:ì´ˆ) í˜•ì‹ìœ¼ë¡œ ê¸°ë¡í•˜ì„¸ìš”.\n",
    "    4. ê° ìš”ì•½ ë¬¸ì¥ì— ì í•©í•œ ì´ëª¨ì§€ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.\n",
    "    5. ìš”ì•½ì€ í•œê¸€ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "    6. ì£¼ì œëŠ” ìµœëŒ€í•œ ë§ì´ ì¶”ì¶œí•´ ì£¼ì„¸ìš”.\n",
    "    7. ì‹œê°„ í‘œê¸°ì— ì˜¤ë¥˜ê°€ ì—†ë„ë¡ ì£¼ì˜í•˜ì„¸ìš”.\n",
    "\n",
    "    ì¶œë ¥ í˜•ì‹:\n",
    "    mm:ss ğŸ”¶ ì£¼ì œì— ëŒ€í•œ í•œ ì¤„ ìš”ì•½\n",
    "    \"\"\"\n",
    "\n",
    "    chunks = chunk_text(instruction)\n",
    "    summaries = []\n",
    "\n",
    "    print(f\"ì´ {len(chunks)}ê°œì˜ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•©ë‹ˆë‹¤.\\n\")\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"\\nì²­í¬ {i+1}/{len(chunks)} ì²˜ë¦¬ ì¤‘...\\n\")\n",
    "        summary = process_chunk(chunk, system_prompt)\n",
    "        summaries.append(summary)\n",
    "        print(summary)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    final_summary = \"\\n\".join(summaries)\n",
    "    formatted_summary = format_summary(final_summary)\n",
    "\n",
    "    return formatted_summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = f\"\"\"ì£¼ì–´ì§„ ë¹„ë””ì˜¤ \"ìë§‰ì •ë³´\" ë¥¼ ë°”íƒ•ìœ¼ë¡œ [ìš”ì²­ì‚¬í•­]ì„ ì°¨ë¡€ëŒ€ë¡œ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.\n",
    "[ìë§‰ì •ë³´]\n",
    "{merged_transcript}\n",
    "\n",
    "[ìš”ì²­ì‚¬í•­]\n",
    "1. ì£¼ì–´ì§„ [ìë§‰ì •ë³´]ì—ì„œ ì¤‘ìš”í•œ ì£¼ì œë¥¼ ì‹œê°„ ìˆœì„œì— ë§ê²Œ ì„ ì •í•˜ê³ , ì£¼ì œê°€ ì‹œì‘ë˜ëŠ” ì‹œê°„ì„ ê¸°ë¡í•´ì£¼ì„¸ìš”.\n",
    "2. ì£¼ì œëŠ” í•œ ì¤„ ìš”ì•½ì„ ì‘ì„±í•˜ê³ , ìë§‰ì—ì„œ ì£¼ì œê°€ ì‹œì‘ë˜ëŠ” ì‹œê°„ì„ mm:ss(ë¶„:ì´ˆ) í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. (ì˜ˆ: 00:05 ğŸ‘‹ ìë§‰ ìƒì„± ê¸°ëŠ¥ì— ëŒ€í•œ ì†Œê°œ)\n",
    "3. ìš”ì•½ì€ í•œê¸€ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.\n",
    "4. ê° ë¬¸ì¥ì— ì í•©í•œ emojië¥¼ ìµœëŒ€í•œ í™œìš©í•´ ì£¼ì„¸ìš”\n",
    "\n",
    "[ì¶œë ¥ì˜ˆì‹œ]\n",
    "00:12 ğŸ‘‹ GPTs ì˜ ì£¼ìš” ê°œë…ì— ëŒ€í•˜ì—¬ ì†Œê°œí•´ìš”\n",
    "03:13 ğŸ‘€ GPTs ì˜ ì¥ë‹¨ì ê³¼ í™œìš© ì‚¬ë¡€ì— ëŒ€í•˜ì—¬ ìì„¸íˆ ì•Œì•„ë´ìš”\n",
    "\n",
    "[ì£¼ì˜ì‚¬í•­]\n",
    "- ì£¼ì œëŠ” ìµœëŒ€í•œ ë§ì´ ì¶”ì¶œí•´ ì£¼ì„¸ìš”.\n",
    "- ì‹œê°„í‘œê¸°ì— ì˜¤ë¥˜ê°€ ì—†ë„ë¡ ì£¼ì˜í•´ ì£¼ì„¸ìš”. ë¶„:ì´ˆ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤. (ì˜ˆ: 00:12)\n",
    "\n",
    "run step-by-step. Take a deep breath. You can do it!\n",
    "\"\"\"\n",
    "summary_output = post_processing(instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
