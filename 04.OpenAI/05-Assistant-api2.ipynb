{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "# API KEY 정보를 불러옵니다\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    " \n",
    "client = OpenAI()\n",
    " \n",
    "assistant = client.beta.assistants.create(\n",
    "  name=\"AI Expert\",\n",
    "  instructions=\"You are an AI expert who can answer any questions about AI.\",\n",
    "  model=\"gpt-4o\",\n",
    "  tools=[{\"type\": \"file_search\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector store caled \"Financial Statements\"\n",
    "vector_store = client.beta.vector_stores.create(name=\"AI paper repository\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: E:\\course\\04.OpenAI\\data\\llm-critics-help-catch-llm-bugs-paper.pdf\n",
      "File size: 1230967 bytes\n"
     ]
    }
   ],
   "source": [
    "# Ready the files for upload to OpenAI\n",
    "file_paths = [\"E:\\\\course\\\\04.OpenAI\\\\data\\\\llm-critics-help-catch-llm-bugs-paper.pdf\"]\n",
    "for path in file_paths:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"File exists: {path}\")\n",
    "        print(f\"File size: {os.path.getsize(path)} bytes\")\n",
    "    else:\n",
    "        print(f\"File does not exist: {path}\")\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the upload and poll SDK helper to upload the files, add them to the vector store,\n",
    "# and poll the status of the file batch for completion.\n",
    "try:\n",
    "    file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "        vector_store_id=vector_store.id, \n",
    "        files=file_streams\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error details: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.update(\n",
    "  assistant_id=assistant.id,\n",
    "  tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToolResourcesFileSearch(vector_store_ids=['vs_IBxcVEWSPFSrdKpvRPhq0ZZt'])\n"
     ]
    }
   ],
   "source": [
    "# Upload the user provided file to OpenAI\n",
    "message_file = client.files.create(\n",
    "  file=open(\"E:\\\\course\\\\04.OpenAI\\\\data\\\\llm-critics-help-catch-llm-bugs-paper.pdf\", \"rb\"), purpose=\"assistants\"\n",
    ")\n",
    "\n",
    "question = \"최근에 OpenAI에서 개발한 AI 모델은 무엇인가요? 상세히 한국어로 설명해주세요.\"\n",
    " \n",
    "# Create a thread and attach the file to the message\n",
    "thread = client.beta.threads.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": question,\n",
    "      # Attach the new file to the message.\n",
    "      \"attachments\": [\n",
    "        { \"file_id\": message_file.id, \"tools\": [{\"type\": \"file_search\"}] }\n",
    "      ],\n",
    "    }\n",
    "  ]\n",
    ")\n",
    " \n",
    "# The thread now has a vector store with that file in its tool resources.\n",
    "print(thread.tool_resources.file_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최근 OpenAI에서 개발한 AI 모델 중 하나는 CriticGPT입니다. 이 모델의 주요 목적은 코드 작성과 관련된 오류를 찾아내는 데 도움을 주는 것입니다. CriticGPT는 인간 피드백을 활용한 강화 학습(RLHF, Reinforcement Learning from Human Feedback)을 통해 훈련된 대형 언어 모델입니다.\n",
      "\n",
      "### 모델 개요\n",
      "CriticGPT는 GPT-4 패밀리의 Transformer 언어 모델을 기반으로 합니다. 이 모델은 자연어 피드백을 작성하여 코드의 문제점을 강조하는 방법으로 훈련됩니다. CriticGPT는 인간 평가자를 도와 모델이 작성한 코드의 오류를 더 정확하게 평가하기 위해 개발되었습니다  .\n",
      "\n",
      "### 성능 및 특징\n",
      "1. **오류 탐지 성능**:\n",
      "   CriticGPT는 인간 평가자보다 코드에서 발생하는 오류를 더 효과적으로 찾아냅니다. 실제 코드에서 발생하는 오류가 포함된 코드에 대해 모델이 작성한 비평은 63%의 경우에서 인간 평가자가 작성한 비평보다 선호되었습니다[0].\n",
      "\n",
      "2. **비교 우위**:\n",
      "   CriticGPT는 ChatGPT와 비교하여 더 높은 정밀도와 재현율을 보여주며, 코드에서 인간이 삽입한 오류와 검출된 오류 모두에서 더 나은 성능을 나타냈습니다. 예를 들어, CriticGPT (RL only) 버전은 Human Inserted Bugs 분포에서 더 나은 성능을 보이며, Human Detected Bugs 분포에서는 ChatGPT보다 더 포괄적인 비평을 제공합니다 [1][2].\n",
      "\n",
      "3. **비경계 요소 감지 성능**:\n",
      "   CriticGPT와 같은 LLM(대규모 언어 모델)은 예상치 못한 오류를 감지하는 데도 유용하며, 특히 ChatGPT 훈련 데이터에서 \"완벽한(flaws)\"으로 분류된 여러 오류를 성공적으로 식별할 수 있습니다. 이는 CriticGPT가 코드 이외의 일반 보조 업무에서도 유용하게 사용될 수 있음을 시사합니다[3].\n",
      "\n",
      "4. **FSBS(Force Sampling Beam Search)**:\n",
      "   CriticGPT는 FSBS 기법을 사용하여 비평의 포괄성을 높이는 동시에 허위 정보를 줄이는 데 도움을 줍니다. FSBS는 모델이 답변의 일부를 선택하도록 강제한 후, 평가 완료된 다양한 비평 중 최고 점수를 받은 비평을 선택하는 방식으로, 비평의 길이와 내용을 최적화합니다[4].\n",
      "\n",
      "이러한 CriticGPT는 AI 모델의 오류를 탐지하고 수정하는 데 중요한 도구로 활용될 수 있으며, 개발자와 연구자들에게 큰 도움이 될 것입니다 [4][6].\n",
      "[0] llm-critics-help-catch-llm-bugs-paper.pdf\n",
      "[1] llm-critics-help-catch-llm-bugs-paper.pdf\n",
      "[2] llm-critics-help-catch-llm-bugs-paper.pdf\n",
      "[3] llm-critics-help-catch-llm-bugs-paper.pdf\n",
      "[4] llm-critics-help-catch-llm-bugs-paper.pdf\n",
      "[5] llm-critics-help-catch-llm-bugs-paper.pdf\n",
      "[6] llm-critics-help-catch-llm-bugs-paper.pdf\n"
     ]
    }
   ],
   "source": [
    "# Use the create and poll SDK helper to create a run and poll the status of\n",
    "# the run until it's in a terminal state.\n",
    "\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id, assistant_id=assistant.id\n",
    ")\n",
    "\n",
    "messages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\n",
    "\n",
    "message_content = messages[0].content[0].text\n",
    "annotations = message_content.annotations\n",
    "citations = []\n",
    "for index, annotation in enumerate(annotations):\n",
    "    message_content.value = message_content.value.replace(annotation.text, f\"[{index}]\")\n",
    "    if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "        cited_file = client.files.retrieve(file_citation.file_id)\n",
    "        citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "print(message_content.value)\n",
    "print(\"\\n\".join(citations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
